<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Keyword Tokenizer
        | Elasticsearch Reference [7.x]
      | Elastic
    </title><link rel="home" href="index.html" title="Elasticsearch Reference [7.x]" /><link rel="up" href="analysis-tokenizers.html" title="Tokenizers" /><link rel="prev" href="max-gram-limits.html" title="Limitations of the max_gram parameter" /><link rel="next" href="analysis-letter-tokenizer.html" title="Letter Tokenizer" /><meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/7.x" /><meta name="DC.subject" content="Elasticsearch" /><meta name="DC.identifier" content="7.x" /></head><body><div class="page_header">You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference
      [7.x]
    </a></span> » <span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span> » <span class="breadcrumb-link"><a href="analysis-tokenizers.html">Tokenizers</a></span> » <span class="breadcrumb-node">Keyword Tokenizer</span></div><div class="navheader"><span class="prev"><a href="max-gram-limits.html">
              « 
              Limitations of the <code class="literal">max_gram</code> parameter</a>
           
        </span><span class="next">
           
          <a href="analysis-letter-tokenizer.html">Letter Tokenizer
               »
            </a></span></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="analysis-keyword-tokenizer"></a>Keyword Tokenizer<a href="https://github.com/elastic/elasticsearch/edit/7.x/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>The <code class="literal">keyword</code> tokenizer  is a <span class="quote">“<span class="quote">noop</span>”</span> tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters to normalise output, e.g. lower-casing email addresses.</p><h3><a id="_example_output_11"></a>Example output<a href="https://github.com/elastic/elasticsearch/edit/7.x/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><div class="pre_wrapper lang-console"><pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "keyword",
  "text": "New York"
}</pre></div><div class="console_widget" data-snippet="snippets/779.console"></div><p>The above sentence would produce the following term:</p><div class="pre_wrapper lang-text"><pre class="programlisting prettyprint lang-text">[ New York ]</pre></div><h3><a id="_configuration_12"></a>Configuration<a href="https://github.com/elastic/elasticsearch/edit/7.x/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>The <code class="literal">keyword</code> tokenizer accepts the following parameters:</p><div class="informaltable"><table cellpadding="4px" border="0"><colgroup><col /><col /></colgroup><tbody valign="top"><tr><td valign="top">
<p><code class="literal">buffer_size</code></p>
</td><td valign="top">
<p>The number of characters read into the term buffer in a single pass.
Defaults to <code class="literal">256</code>.  The term buffer will grow by this size until all the
text has been consumed.  It is advisable not to change this setting.</p>
</td></tr></tbody></table></div></div><div class="navfooter"><span class="prev"><a href="max-gram-limits.html">
              « 
              Limitations of the <code class="literal">max_gram</code> parameter</a>
           
        </span><span class="next">
           
          <a href="analysis-letter-tokenizer.html">Letter Tokenizer
               »
            </a></span></div></body></html>
