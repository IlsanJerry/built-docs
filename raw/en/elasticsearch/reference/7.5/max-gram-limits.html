<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Limitations of the max_gram parameter
        | Elasticsearch Reference [7.5]
      | Elastic
    </title><link rel="home" href="index.html" title="Elasticsearch Reference [7.5]" /><link rel="up" href="analysis-tokenizers.html" title="Tokenizers" /><link rel="prev" href="analysis-edgengram-tokenizer.html" title="Edge n-gram tokenizer" /><link rel="next" href="analysis-keyword-tokenizer.html" title="Keyword Tokenizer" /><meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/7.5" /><meta name="DC.subject" content="Elasticsearch" /><meta name="DC.identifier" content="7.5" /></head><body><div class="page_header">You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference
      [7.5]
    </a></span> » <span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span> » <span class="breadcrumb-link"><a href="analysis-tokenizers.html">Tokenizers</a></span> » <span class="breadcrumb-node">Limitations of the <code class="literal">max_gram</code> parameter</span></div><div class="navheader"><span class="prev"><a href="analysis-edgengram-tokenizer.html">
              « 
              Edge n-gram tokenizer</a>
           
        </span><span class="next">
           
          <a href="analysis-keyword-tokenizer.html">Keyword Tokenizer
               »
            </a></span></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="max-gram-limits"></a>Limitations of the <code class="literal">max_gram</code> parameter<a href="https://github.com/elastic/elasticsearch/edit/7.5/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>The <code class="literal">edge_ngram</code> tokenizer’s <code class="literal">max_gram</code> value limits the character length of
tokens. When the <code class="literal">edge_ngram</code> tokenizer is used with an index analyzer, this
means search terms longer than the <code class="literal">max_gram</code> length may not match any indexed
terms.</p><p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code>, searches for <code class="literal">apple</code> won’t match the
indexed term <code class="literal">app</code>.</p><p>To account for this, you can use the
<a class="link" href="analysis-truncate-tokenfilter.html" title="Truncate Token Filter"><code class="literal">truncate</code></a> token filter with a search analyzer
to shorten search terms to the <code class="literal">max_gram</code> character length. However, this could
return irrelevant results.</p><p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code> and search terms are truncated to three
characters, the search term <code class="literal">apple</code> is shortened to <code class="literal">app</code>. This means searches
for <code class="literal">apple</code> return any indexed terms matching <code class="literal">app</code>, such as <code class="literal">apply</code>, <code class="literal">snapped</code>,
and <code class="literal">apple</code>.</p><p>We recommend testing both approaches to see which best fits your
use case and desired search experience.</p><h3><a id="_example_configuration_7"></a>Example configuration<a href="https://github.com/elastic/elasticsearch/edit/7.5/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>In this example, we configure the <code class="literal">edge_ngram</code> tokenizer to treat letters and
digits as tokens, and to produce grams with minimum length <code class="literal">2</code> and maximum
length <code class="literal">10</code>:</p><div class="pre_wrapper lang-console"><pre class="programlisting prettyprint lang-console">PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}</pre></div><div class="console_widget" data-snippet="snippets/768.console"></div><p>The above example produces the following terms:</p><div class="pre_wrapper lang-text"><pre class="programlisting prettyprint lang-text">[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]</pre></div><p>Usually we recommend using the same <code class="literal">analyzer</code> at index time and at search
time. In the case of the <code class="literal">edge_ngram</code> tokenizer, the advice is different. It
only makes sense to use the <code class="literal">edge_ngram</code> tokenizer at index time, to ensure
that partial words are available for matching in the index. At search time,
just search for the terms the user has typed in, for instance: <code class="literal">Quick Fo</code>.</p><p>Below is an example of how to set up a field for <span class="emphasis"><em>search-as-you-type</em></span>.</p><p>Note that the <code class="literal">max_gram</code> value for the index analyzer is <code class="literal">10</code>, which limits
indexed terms to 10 characters. Search terms are not truncated, meaning that
search terms longer than 10 characters may not match any indexed terms.</p><div class="pre_wrapper lang-console"><pre class="programlisting prettyprint lang-console">PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete": {
          "tokenizer": "autocomplete",
          "filter": [
            "lowercase"
          ]
        },
        "autocomplete_search": {
          "tokenizer": "lowercase"
        }
      },
      "tokenizer": {
        "autocomplete": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "autocomplete",
        "search_analyzer": "autocomplete_search"
      }
    }
  }
}

PUT my_index/_doc/1
{
  "title": "Quick Foxes" <a id="CO267-1"></a><i class="conum" data-value="1"></i>
}

POST my_index/_refresh

GET my_index/_search
{
  "query": {
    "match": {
      "title": {
        "query": "Quick Fo", <a id="CO267-2"></a><i class="conum" data-value="2"></i>
        "operator": "and"
      }
    }
  }
}</pre></div><div class="console_widget" data-snippet="snippets/769.console"></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO267-1"><i class="conum" data-value="1"></i></a> </p></td><td valign="top" align="left"><p>The <code class="literal">autocomplete</code> analyzer indexes the terms <code class="literal">[qu, qui, quic, quick, fo, fox, foxe, foxes]</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO267-2"><i class="conum" data-value="2"></i></a> </p></td><td valign="top" align="left"><p>The <code class="literal">autocomplete_search</code> analyzer searches for the terms <code class="literal">[quick, fo]</code>, both of which appear in the index.</p></td></tr></table></div></div><div class="navfooter"><span class="prev"><a href="analysis-edgengram-tokenizer.html">
              « 
              Edge n-gram tokenizer</a>
           
        </span><span class="next">
           
          <a href="analysis-keyword-tokenizer.html">Keyword Tokenizer
               »
            </a></span></div></body></html>
