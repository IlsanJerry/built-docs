<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>standard Tokenizer
        | Elasticsearch: The Definitive Guide [2.x]
      | Elastic
    </title><link rel="home" href="index.html" title="Elasticsearch: The Definitive Guide [2.x]" /><link rel="up" href="identifying-words.html" title="Identifying Words" /><link rel="prev" href="standard-analyzer.html" title="standard Analyzer" /><link rel="next" href="icu-plugin.html" title="Installing the ICU Plug-in" /><meta name="DC.type" content="Learn/Docs/Legacy/Elasticsearch/Definitive Guide/2.x" /><meta name="DC.subject" content="Elasticsearch" /><meta name="DC.identifier" content="2.x" /><meta name="robots" content="noindex,nofollow" /></head><body><div class="page_header">This information applies to version 2.x of Elasticsearch. For the
most up to date information, see the current version of the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">
Elasticsearch Reference</a>.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch: The Definitive Guide
      [2.x]
    </a></span> » <span class="breadcrumb-link"><a href="languages.html">Dealing with Human Language</a></span> » <span class="breadcrumb-link"><a href="identifying-words.html">Identifying Words</a></span> » <span class="breadcrumb-node">standard Tokenizer</span></div><div class="navheader"><span class="prev"><a href="standard-analyzer.html">
              « 
              standard Analyzer</a>
           
        </span><span class="next">
           
          <a href="icu-plugin.html">Installing the ICU Plug-in
               »
            </a></span></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="standard-tokenizer"></a>standard Tokenizer<a href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/210_Identifying_words/20_Standard_tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>A <span class="emphasis"><em>tokenizer</em></span> accepts a string as input, processes<a id="id-1.6.4.8.2.2" class="indexterm"></a>
<a id="id-1.6.4.8.2.3" class="indexterm"></a>
<a id="id-1.6.4.8.2.4" class="indexterm"></a><a id="id-1.6.4.8.2.5" class="indexterm"></a><a id="id-1.6.4.8.2.6" class="indexterm"></a> the string to break it
into individual words, or <span class="emphasis"><em>tokens</em></span> (perhaps discarding some characters like
punctuation), and emits a <span class="emphasis"><em>token stream</em></span> as output.</p><p>What is interesting is the algorithm that is used to <span class="emphasis"><em>identify</em></span> words. The
<code class="literal">whitespace</code> tokenizer <a id="id-1.6.4.8.3.3" class="indexterm"></a>simply breaks on whitespace—​spaces, tabs, line
feeds, and so forth—​and assumes that contiguous nonwhitespace characters form a
single token. For instance:</p><div class="pre_wrapper lang-js"><pre class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=whitespace
You're the 1st runner home!</pre></div><p>This request would return the following terms:
<code class="literal">You're</code>, <code class="literal">the</code>, <code class="literal">1st</code>, <code class="literal">runner</code>, <code class="literal">home!</code></p><p>The <code class="literal">letter</code> tokenizer, on the other hand, breaks on any character that is
not a letter, and so would <a id="id-1.6.4.8.6.2" class="indexterm"></a>return the following terms: <code class="literal">You</code>, <code class="literal">re</code>, <code class="literal">the</code>,
<code class="literal">st</code>, <code class="literal">runner</code>, <code class="literal">home</code>.</p><p>The <code class="literal">standard</code> tokenizer<a id="id-1.6.4.8.7.2" class="indexterm"></a> uses the Unicode Text Segmentation algorithm (as
defined in <a class="ulink" href="http://unicode.org/reports/tr29/" target="_top">Unicode Standard Annex #29</a>) to
find the boundaries <span class="emphasis"><em>between</em></span> words,<a id="id-1.6.4.8.7.5" class="indexterm"></a> and emits everything in-between. Its
knowledge of Unicode allows it to successfully tokenize text containing a
mixture of languages.</p><p>Punctuation may<a id="id-1.6.4.8.8.1" class="indexterm"></a>
<a id="id-1.6.4.8.8.2" class="indexterm"></a> or may not be considered part of a word, depending on
where it appears:</p><div class="pre_wrapper lang-js"><pre class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=standard
You're my 'favorite'.</pre></div><p>In this example, the apostrophe in <code class="literal">You're</code> is treated as part of the
word, while the single quotes in <code class="literal">'favorite'</code> are not, resulting in the
following terms: <code class="literal">You're</code>, <code class="literal">my</code>, <code class="literal">favorite</code>.</p><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>The <code class="literal">uax_url_email</code> tokenizer works<a id="id-1.6.4.8.11.1.2" class="indexterm"></a> in exactly the same way as the <code class="literal">standard</code>
tokenizer, except that it recognizes<a id="id-1.6.4.8.11.1.4" class="indexterm"></a> email addresses and URLs and emits them as
single tokens. The <code class="literal">standard</code> tokenizer, on the other hand, would try to
break them into individual words. For instance, the email address
<code class="literal">joe-bloggs@foo-bar.com</code> would result in the tokens <code class="literal">joe</code>, <code class="literal">bloggs</code>, <code class="literal">foo</code>,
<code class="literal">bar.com</code>.</p></div></div><p>The <code class="literal">standard</code> tokenizer is a reasonable starting point for tokenizing most
languages, especially Western languages.  In fact, it forms the basis of most
of the language-specific analyzers like the <code class="literal">english</code>, <code class="literal">french</code>, and <code class="literal">spanish</code>
analyzers. Its support for Asian languages, however, is limited, and you should consider
using the <code class="literal">icu_tokenizer</code> instead,<a id="id-1.6.4.8.12.6" class="indexterm"></a> which is available in the ICU plug-in.</p></div><div class="navfooter"><span class="prev"><a href="standard-analyzer.html">
              « 
              standard Analyzer</a>
           
        </span><span class="next">
           
          <a href="icu-plugin.html">Installing the ICU Plug-in
               »
            </a></span></div></body></html>
