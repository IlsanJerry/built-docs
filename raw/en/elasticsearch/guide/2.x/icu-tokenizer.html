<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>icu_tokenizer
        | Elasticsearch: The Definitive Guide [2.x]
      | Elastic
    </title><link rel="home" href="index.html" title="Elasticsearch: The Definitive Guide [2.x]" /><link rel="up" href="identifying-words.html" title="Identifying Words" /><link rel="prev" href="icu-plugin.html" title="Installing the ICU Plug-in" /><link rel="next" href="char-filters.html" title="Tidying Up Input Text" /><meta name="DC.type" content="Learn/Docs/Legacy/Elasticsearch/Definitive Guide/2.x" /><meta name="DC.subject" content="Elasticsearch" /><meta name="DC.identifier" content="2.x" /><meta name="robots" content="noindex,nofollow" /></head><body><div class="page_header">This information applies to version 2.x of Elasticsearch. For the
most up to date information, see the current version of the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">
Elasticsearch Reference</a>.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch: The Definitive Guide
      [2.x]
    </a></span> » <span class="breadcrumb-link"><a href="languages.html">Dealing with Human Language</a></span> » <span class="breadcrumb-link"><a href="identifying-words.html">Identifying Words</a></span> » <span class="breadcrumb-node">icu_tokenizer</span></div><div class="navheader"><span class="prev"><a href="icu-plugin.html">
              « 
              Installing the ICU Plug-in</a>
           
        </span><span class="next">
           
          <a href="char-filters.html">Tidying Up Input Text
               »
            </a></span></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="icu-tokenizer"></a>icu_tokenizer<a href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/210_Identifying_words/40_ICU_tokenizer.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>The <code class="literal">icu_tokenizer</code> uses the same Unicode Text Segmentation algorithm as the
<code class="literal">standard</code> tokenizer,<a id="id-1.6.4.10.2.3" class="indexterm"></a>
<a id="id-1.6.4.10.2.4" class="indexterm"></a>
<a id="id-1.6.4.10.2.5" class="indexterm"></a><a id="id-1.6.4.10.2.6" class="indexterm"></a><a id="id-1.6.4.10.2.7" class="indexterm"></a> but adds better support for some Asian languages by
using a dictionary-based approach to identify words in Thai, Lao, Chinese,
Japanese, and Korean, and using custom rules to break Myanmar and Khmer text
into syllables.</p><p>For instance, compare the tokens <a id="id-1.6.4.10.3.1" class="indexterm"></a>
<a id="id-1.6.4.10.3.2" class="indexterm"></a>produced by the <code class="literal">standard</code> and
<code class="literal">icu_tokenizers</code>, respectively, when tokenizing <span class="quote">“<span class="quote">Hello. I am from Bangkok.</span>”</span> in
Thai:</p><div class="pre_wrapper lang-js"><pre class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=standard
สวัสดี ผมมาจากกรุงเทพฯ</pre></div><p>The <code class="literal">standard</code> tokenizer produces two tokens, one for each sentence: <code class="literal">สวัสดี</code>,
<code class="literal">ผมมาจากกรุงเทพฯ</code>.  That is useful only if you want to search for the whole
sentence <span class="quote">“<span class="quote">I am from Bangkok.</span>”</span>, but not if you want to search for just
<span class="quote">“<span class="quote">Bangkok.</span>”</span></p><div class="pre_wrapper lang-js"><pre class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=icu_tokenizer
สวัสดี ผมมาจากกรุงเทพฯ</pre></div><p>The <code class="literal">icu_tokenizer</code>, on the other hand, is able to break up the text into the
individual words (<code class="literal">สวัสดี</code>, <code class="literal">ผม</code>, <code class="literal">มา</code>, <code class="literal">จาก</code>, <code class="literal">กรุงเทพฯ)</code>, making them
easier to search.</p><p>In contrast, the <code class="literal">standard</code> tokenizer <span class="quote">“<span class="quote">over-tokenizes</span>”</span> Chinese and Japanese
text, often breaking up whole words into single characters. Because there
are no spaces between words, it can be difficult to tell whether consecutive
characters are separate words or form a single word.  For instance:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">向 means <span class="emphasis"><em>facing</em></span>, 日 means <span class="emphasis"><em>sun</em></span>, and 葵 means <span class="emphasis"><em>hollyhock</em></span>. When
written together, 向日葵 means <span class="emphasis"><em>sunflower</em></span>.</li><li class="listitem">五 means <span class="emphasis"><em>five</em></span> or <span class="emphasis"><em>fifth</em></span>, 月 means <span class="emphasis"><em>month</em></span>, and 雨 means <span class="emphasis"><em>rain</em></span>.
The first two characters written together as 五月 mean <span class="emphasis"><em>the month
of May</em></span>, and adding the third character, 五月雨 means
<span class="emphasis"><em>continuous rain</em></span>. When combined with a fourth character, 式,
meaning <span class="emphasis"><em>style</em></span>, the word 五月雨式 becomes an adjective for anything
consecutive or unrelenting.</li></ul></div><p>Although each character may be a word in its own right, tokens are more
meaningful when they retain the bigger original concept instead of just the
component parts:</p><div class="pre_wrapper lang-js"><pre class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=standard
向日葵

GET /_analyze?tokenizer=icu_tokenizer
向日葵</pre></div><p>The <code class="literal">standard</code> tokenizer in the preceding example would emit each character
as a separate token: <code class="literal">向</code>, <code class="literal">日</code>, <code class="literal">葵</code>. The <code class="literal">icu_tokenizer</code> would
emit the single token <code class="literal">向日葵</code> (sunflower).</p><p>Another difference between the <code class="literal">standard</code> tokenizer and the <code class="literal">icu_tokenizer</code> is
that the latter will break a word containing characters written in different
scripts (for example, <code class="literal">βeta</code>) into separate tokens—<code class="literal">β</code>, <code class="literal">eta</code>—while the
former will emit the word as a single token: <code class="literal">βeta</code>.</p></div><div class="navfooter"><span class="prev"><a href="icu-plugin.html">
              « 
              Installing the ICU Plug-in</a>
           
        </span><span class="next">
           
          <a href="char-filters.html">Tidying Up Input Text
               »
            </a></span></div></body></html>
