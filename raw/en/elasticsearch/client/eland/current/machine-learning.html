<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Machine Learning | Eland Python Client | Elastic</title>
<link rel="home" href="index.html" title="Eland Python Client"/>
<link rel="up" href="index.html" title="Eland Python Client"/>
<link rel="prev" href="dataframes.html" title="Data Frames"/>
<meta name="DC.type" content="Learn/Docs/Clients/eland"/>
<meta name="DC.subject" content="Clients"/>
<meta name="DC.identifier" content="master"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Eland Python Client</a></span>
»
<span class="breadcrumb-node">Machine Learning</span>
</div>
<div class="navheader">
<span class="prev">
<a href="dataframes.html">« Data Frames</a>
</span>
<span class="next">
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="machine-learning"></a>Machine Learning<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h1>
</div></div></div>
<h3><a id="ml-trained-models"></a>Trained models<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h3>
<p>Eland allows transforming trained models from scikit-learn, XGBoost,
and LightGBM libraries to be serialized and used as an inference
model in Elasticsearch.</p>
<div class="pre_wrapper lang-python">
<pre class="programlisting prettyprint lang-python">&gt;&gt;&gt; from xgboost import XGBClassifier
&gt;&gt;&gt; from eland.ml import MLModel

# Train and exercise an XGBoost ML model locally
&gt;&gt;&gt; xgb_model = XGBClassifier(booster="gbtree")
&gt;&gt;&gt; xgb_model.fit(training_data[0], training_data[1])

&gt;&gt;&gt; xgb_model.predict(training_data[0])
[0 1 1 0 1 0 0 0 1 0]

# Import the model into Elasticsearch
&gt;&gt;&gt; es_model = MLModel.import_model(
    es_client="http://localhost:9200",
    model_id="xgb-classifier",
    model=xgb_model,
    feature_names=["f0", "f1", "f2", "f3", "f4"],
)

# Exercise the ML model in Elasticsearch with the training data
&gt;&gt;&gt; es_model.predict(training_data[0])
[0 1 1 0 1 0 0 0 1 0]</pre>
</div>
<h3><a id="ml-nlp-pytorch"></a>Natural language processing (NLP) with PyTorch<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h3>
<p>For NLP tasks, Eland enables you to import PyTorch trained BERT models into Elasticsearch.
Models can be either plain PyTorch models, or supported
<a href="https://huggingface.co/transformers" class="ulink" target="_top">transformers</a> models from the
<a href="https://huggingface.co/models" class="ulink" target="_top">Hugging Face model hub</a>.</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">$ eland_import_hub_model \
  --url http://localhost:9200/ \
  --hub-model-id elastic/distilbert-base-cased-finetuned-conll03-english \
  --task-type ner \
  --start</pre>
</div>
<div class="pre_wrapper lang-python">
<pre class="programlisting prettyprint lang-python">&gt;&gt;&gt; import elasticsearch
&gt;&gt;&gt; from pathlib import Path
&gt;&gt;&gt; from eland.ml.pytorch import PyTorchModel
&gt;&gt;&gt; from eland.ml.pytorch.transformers import TransformerModel

# Load a Hugging Face transformers model directly from the model hub
&gt;&gt;&gt; tm = TransformerModel("elastic/distilbert-base-cased-finetuned-conll03-english", "ner")
Downloading: 100%|██████████| 257/257 [00:00&lt;00:00, 108kB/s]
Downloading: 100%|██████████| 954/954 [00:00&lt;00:00, 372kB/s]
Downloading: 100%|██████████| 208k/208k [00:00&lt;00:00, 668kB/s]
Downloading: 100%|██████████| 112/112 [00:00&lt;00:00, 43.9kB/s]
Downloading: 100%|██████████| 249M/249M [00:23&lt;00:00, 11.2MB/s]

# Export the model in a TorchScrpt representation which Elasticsearch uses
&gt;&gt;&gt; tmp_path = "models"
&gt;&gt;&gt; Path(tmp_path).mkdir(parents=True, exist_ok=True)
&gt;&gt;&gt; model_path, config_path, vocab_path = tm.save(tmp_path)

# Import model into Elasticsearch
&gt;&gt;&gt; es = elasticsearch.Elasticsearch("http://elastic:mlqa_admin@localhost:9200", timeout=300)  # 5 minute timeout
&gt;&gt;&gt; ptm = PyTorchModel(es, tm.elasticsearch_model_id())
&gt;&gt;&gt; ptm.import_model(model_path, config_path, vocab_path)
100%|██████████| 63/63 [00:12&lt;00:00,  5.02it/s]</pre>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="dataframes.html">« Data Frames</a>
</span>
<span class="next">
</span>
</div>
</div>
</body>
</html>
