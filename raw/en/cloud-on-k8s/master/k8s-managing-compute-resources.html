<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Managing compute resources
        | Elastic Cloud on Kubernetes [master]
      | Elastic
    </title><link rel="home" href="index.html" title="Elastic Cloud on Kubernetes [master]" /><link rel="up" href="index.html" title="Elastic Cloud on Kubernetes [master]" /><link rel="prev" href="k8s-accessing-elastic-services.html" title="Accessing Elastic Stack services" /><link rel="next" href="k8s-elasticsearch-specification.html" title="Running Elasticsearch on ECK" /><meta name="DC.type" content="Learn/Docs/Kubernetes/Reference/master" /><meta name="DC.subject" content="ECK" /><meta name="DC.identifier" content="master" /></head><body><div class="page_header">You are looking at the documentation for a beta release.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elastic Cloud on Kubernetes
      [master]
    </a></span> » <span class="breadcrumb-node">Managing compute resources</span></div><div class="navheader"><span class="prev"><a href="k8s-accessing-elastic-services.html">
              « 
              Accessing Elastic Stack services</a>
           
        </span><span class="next">
           
          <a href="k8s-elasticsearch-specification.html">Running Elasticsearch on ECK
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="k8s-managing-compute-resources"></a>Managing compute resources<a href="https://github.com/elastic/cloud-on-k8s/edit/master/docs/managing-compute-resources.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h1></div></div></div><p>In order to help the Kubernetes scheduler make better decisions about how to place pods in available Kubernetes nodes and ensure quality of service (QoS), it is recommended to specify the CPU and memory requirements for objects managed by the operator (Elasticsearch, Kibana or APM Server). In Kubernetes parlance, <code class="literal">requests</code> defines the minimum amount of resources that must be available for a pod to be scheduled and <code class="literal">limits</code> defines the maximum amount of resources that a pod is allowed to consume. For more information about how Kubernetes uses these hints, see: <a class="ulink" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/" target="_top">Managing Compute Resources for Containers</a>.</p><h3><a id="k8s-compute-resources"></a>Set compute resources<a href="https://github.com/elastic/cloud-on-k8s/edit/master/docs/managing-compute-resources.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>Compute resource constraints can be set in the <code class="literal">podTemplate</code> of objects managed by the operator.</p><h4><a id="k8s-compute-resources-elasticsearch"></a>Set compute resources for Elasticsearch<a href="https://github.com/elastic/cloud-on-k8s/edit/master/docs/managing-compute-resources.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For Elasticsearch objects, it is important to consider the heap size when setting resource requirements. The recommendation for heap size is that it should be half the size of RAM allocated to the pod. To minimize disruption caused by pod evictions due to resource contention, it is highly recommended to run Elasticsearch pods at the "Guaranteed" QoS level by setting both <code class="literal">requests</code> and <code class="literal">limits</code> to appropriate values. It is also worth bearing in mind that Kubernetes throttles containers exceeding the CPU limit defined in the <code class="literal">limits</code> section. Do not set this value too low or it would affect the performance of Elasticsearch even if you have enough resources available in the Kubernetes cluster.</p><div class="pre_wrapper lang-yaml"><pre class="programlisting prettyprint lang-yaml">spec:
  nodeSets:
  - podTemplate:
      spec:
        containers:
        - name: elasticsearch
          env:
          - name: ES_JAVA_OPTS
            value: -Xms2g -Xmx2g
          resources:
            requests:
              memory: 4Gi
              cpu: 0.5
            limits:
              memory: 4Gi
              cpu: 2</pre></div><h4><a id="k8s-compute-resources-kibana-and-apm"></a>Set compute resources for Kibana and APM Server<a href="https://github.com/elastic/cloud-on-k8s/edit/master/docs/managing-compute-resources.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For Kibana or APM Server objects, the <code class="literal">podTemplate</code> can be configured as follows:</p><div class="pre_wrapper lang-yaml"><pre class="programlisting prettyprint lang-yaml">spec:
  podTemplate:
    spec:
      containers:
      - name: kibana <a id="CO1-1"></a><i class="conum" data-value="1"></i>
        resources:
          requests:
            memory: 1Gi
            cpu: 0.5
          limits:
            memory: 2Gi
            cpu: 2</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><i class="conum" data-value="1"></i></a> </p></td><td valign="top" align="left"><p>Replace with <code class="literal">kibana</code> or <code class="literal">apm-server</code> as appropriate.</p></td></tr></table></div><h3><a id="k8s-default-behavior"></a>Default behavior<a href="https://github.com/elastic/cloud-on-k8s/edit/master/docs/managing-compute-resources.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>If <code class="literal">resources</code> is not defined in the specification of an object, then the operator applies a default memory limit to ensure that pods have enough resources to start correctly. As the operator cannot make assumptions about the available CPU resources in the cluster, no CPU limits will be set — resulting in the pods having the "Burstable" QoS class. You should consider whether this is acceptable for your use case and follow the instructions in the <a class="xref" href="k8s-managing-compute-resources.html#k8s-compute-resources" title="Set compute resources">Set compute resources</a> section to configure appropriate limits.</p><div class="table"><a id="id-1.6.14"></a><p class="title"><strong>Table 1. Default limits applied by the operator</strong></p><div class="table-contents"><table summary="Default limits applied by the operator" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Type</th><th align="left" valign="top">Requests</th><th align="left" valign="top">Limits</th></tr></thead><tbody><tr><td align="left" valign="top"><p><span class="strong strong"><strong>APM Server</strong></span></p></td><td align="left" valign="top"><p><code class="literal">512Mi</code></p></td><td align="left" valign="top"><p><code class="literal">512Mi</code></p></td></tr><tr><td align="left" valign="top"><p><span class="strong strong"><strong>Elasticsearch</strong></span></p></td><td align="left" valign="top"><p><code class="literal">2Gi</code></p></td><td align="left" valign="top"><p><code class="literal">2Gi</code></p></td></tr><tr><td align="left" valign="top"><p><span class="strong strong"><strong>Kibana</strong></span></p></td><td align="left" valign="top"><p><code class="literal">1Gi</code></p></td><td align="left" valign="top"><p><code class="literal">1Gi</code></p></td></tr></tbody></table></div></div><p>If the Kubernetes cluster is configured with <a class="ulink" href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/" target="_top">LimitRanges</a> that enforce a minimum memory constraint, they could interfere with the operator defaults and cause object creation to fail.</p><p>For example, you might have a LimitRange that enforces a default and minimum memory limit on containers as follows:</p><div class="pre_wrapper lang-yaml"><pre class="programlisting prettyprint lang-yaml">apiVersion: v1
kind: LimitRange
metadata:
  name: default-mem-per-container
spec:
  limits:
  - min:
      memory: "3Gi"
    defaultRequest:
      memory: "3Gi"
    type: Container</pre></div><p>With the above restriction in place, if you attempt to create an Elasticsearch object without defining the <code class="literal">resources</code> section, it will fail to start with an error similar to the following:</p><pre class="literallayout">Cannot create pod elasticsearch-sample-es-ldbgj48c7r: pods "elasticsearch-sample-es-ldbgj48c7r" is forbidden: minimum memory usage per Container is 3Gi, but request is 2Gi</pre><p>This error can be avoided by defining an empty <code class="literal">limits</code> section in the specification to hint to the operator that it should not apply the default limits to the object:</p><div class="pre_wrapper lang-yaml"><pre class="programlisting prettyprint lang-yaml">spec:
  nodeSets:
  - podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            # specify empty limits
            limits: {}</pre></div></div><div class="navfooter"><span class="prev"><a href="k8s-accessing-elastic-services.html">
              « 
              Accessing Elastic Stack services</a>
           
        </span><span class="next">
           
          <a href="k8s-elasticsearch-specification.html">Running Elasticsearch on ECK
               »
            </a></span></div></body></html>
