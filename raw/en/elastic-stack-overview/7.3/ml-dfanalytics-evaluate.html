<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title xmlns="">Evaluating data frame analytics
        | Elastic Stack Overview [7.3]
      | Elastic
    </title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elastic Stack Overview [7.3]" /><link rel="up" href="ml-dfanalytics.html" title="Machine learning data frame analytics" /><link rel="prev" href="dfa-outlier-detection.html" title="Outlier detection" /><link rel="next" href="ml-dfanalytics-apis.html" title="API quick reference" /><meta xmlns="" name="description" content="Get started with the documentation for Elasticsearch, Kibana, Logstash, Beats, X-Pack, Elastic Cloud, Elasticsearch for Apache Hadoop, and our language clients." /><meta xmlns="" name="DC.type" content="Learn/Docs/Elastic Stack/Overview/7.3" /><meta xmlns="" name="DC.subject" content="Elastic Stack" /><meta xmlns="" name="DC.identifier" content="7.3" /></head><body><div xmlns="" class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elastic Stack Overview
      [7.3]
    </a></span> » <span class="breadcrumb-link"><a href="ml-dfanalytics.html">Machine learning data frame analytics</a></span> » <span class="breadcrumb-node">Evaluating data frame analytics</span></div><div xmlns="" class="navheader"><span class="prev"><a href="dfa-outlier-detection.html">
              « 
              Outlier detection</a>
           
        </span><span class="next">
           
          <a href="ml-dfanalytics-apis.html">API quick reference
               »
            </a></span></div><div class="xpack chapter"><div class="titlepage"><div><div><h2 class="title"><a id="ml-dfanalytics-evaluate"></a>Evaluating data frame analytics<a xmlns="" href="https://github.com/elastic/stack-docs/edit/7.3/docs/en/stack/ml/df-analytics/evaluatedf-api.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a><a xmlns="" class="xpack_tag" href="/subscriptions"></a></h2></div></div></div><p>Using the data frame analytics features to gain insights from a data set is an
iterative process. You might need to experiment with different analyses,
parameters, and ways to transform data before you arrive at a result that satisfies
your use case. A valuable companion to this process is the
<a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/7.3/evaluate-dfanalytics.html" target="_top">evaluate data frame analytics API</a>, which enables you to evaluate
the data frame analytics performance against a marked up data set. It helps you
understand error distributions and identifies the points where the data frame analytics
model performs well or less trustworthily.</p><p>The evaluate data frame analytics API is designed for providing a general evaluation mechanism
for the different kinds of data frame analytics. For example, you can evaluate the
results of an outlier detection analysis by using binary soft classification.</p><p>To evaluate the data frame analytics with this API, you need to annotate your index
that contains the results of the analysis with a field that marks each
data frame row with the ground truth. For example, in case of outlier detection,
the field must indicate whether the given data point really is an outlier or
not. The evaluate data frame analytics API evaluates the performance of the data frame analytics against
this manually provided ground truth.</p><h3><a id="ml-dfanalytics-binary-soft-classification"></a>Binary soft classification evaluation<a xmlns="" href="https://github.com/elastic/stack-docs/edit/7.3/docs/en/stack/ml/df-analytics/evaluatedf-api.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>This evaluation type is suitable for analyses which calculate a probability that
each data point in a data set is a member of a class or not. The binary soft classification
evaluation type offers the following metrics to evaluate the model performance:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">confusion matrix</li><li class="listitem">precision</li><li class="listitem">recall</li><li class="listitem">receiver operating characteristic (ROC) curve.</li></ul></div><h4><a id="ml-dfanalytics-confusion-matrix"></a>Confusion matrix<a xmlns="" href="https://github.com/elastic/stack-docs/edit/7.3/docs/en/stack/ml/df-analytics/evaluatedf-api.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>A confusion matrix provides four measures of how well the data frame analytics worked
on your data set:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">True positives (TP): Class members that the analysis identified as class
members.</li><li class="listitem">True negatives (TN): Not class members that the analysis identified as not
class members.</li><li class="listitem">False positives (FP): Not class members that the analysis misidentified as
class members.</li><li class="listitem">False negatives (FN): Class members that the analysis misidentified as not
class members.</li></ul></div><p>Although, the evaluate data frame analytics API can compute the confusion matrix out of the
analysis results, these results are not binary values (class member/not
class member), but a number between 0 and 1 (which called the outlier score in case
of outlier detection). This value captures how likely it is for a data
point to be a member of a certain class. It means that it is up to the user who
is evaluating the results to decide what is the threshold or cutoff point at
which the data point will be considered as a member of the given class. For
example, in the case of outlier detection the user can say that all the data points
with an outlier score higher than 0.5 will be considered as outliers.</p><p>To take this complexity into account, the evaluate data frame analytics API returns the confusion
matrix at different thresholds (by default, 0.25, 0.5, and 0.75).</p><h4><a id="ml-dfanalytics-precision-recall"></a>Precision and recall<a xmlns="" href="https://github.com/elastic/stack-docs/edit/7.3/docs/en/stack/ml/df-analytics/evaluatedf-api.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>A confusion matrix is a useful measure, but it could be hard to compare the
results across the different algorithms. Precision and recall values
summarize the algorithm performance as a single number that makes it easier to
compare the evaluation results.</p><p>Precision shows how many of the data points that the algorithm identified as
class members were actually class members. It is the number of true positives
divided by the sum of the true positives and false positives (TP/(TP+FP)).</p><p>Recall answers a slightly different question. This value shows how many of the
data points that are actual class members were identified correctly as class
members. It is the number of true positives divided by the sum of the true
positives and false negatives (TP/(TP+FN)).</p><p>As was the case for the confusion matrix, you also need to define different
threshold levels for computing precision and recall.</p><h4><a id="ml-dfanalytics-roc"></a>Receiver operating characteristic curve<a xmlns="" href="https://github.com/elastic/stack-docs/edit/7.3/docs/en/stack/ml/df-analytics/evaluatedf-api.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>The receiver operating characteristic (ROC) curve is a plot that represents the
performance of the binary classification process at different thresholds. It
compares the rate of true positives against the rate of false positives at the
different threshold levels to create the curve. From this plot, you can compute
the area under the curve (AUC) value, which is a number between 0 and 1. The
closer to 1, the better the algorithm performance.</p><p>The evaluate data frame analytics API can return the false positive rate (<code class="literal">fpr</code>) and the true
positive rate (<code class="literal">tpr</code>) at the different threshold levels, so you can visualize
the algorithm performance by using these values.</p></div><div xmlns="" class="navfooter"><span class="prev"><a href="dfa-outlier-detection.html">
              « 
              Outlier detection</a>
           
        </span><span class="next">
           
          <a href="ml-dfanalytics-apis.html">API quick reference
               »
            </a></span></div></body></html>