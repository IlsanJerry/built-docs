<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Getting started with the Elastic web crawler | Elastic Enterprise Search Documentation [master] | Elastic</title>
<link rel="home" href="index.html" title="Elastic Enterprise Search Documentation [master]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler.html" title="Elastic web crawler"/>
<link rel="next" href="crawler-searching.html" title="Searching your crawled documents"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/master"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="index.html">Elastic Enterprise Search Documentation [master]</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-searching.html">Searching your crawled documents »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-getting-started"></a>Getting started with the Elastic web crawler<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h2>
</div></div></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Looking for the <span class="strong strong"><strong>App Search web crawler</strong></span>?
See <a href="/guide/en/app-search/master/web-crawler.html" class="ulink" target="_top">the App Search documentation.</a></p>
</div>
</div>
<p>This page walks you through your first targeted test crawls using the Kibana UI.</p>
<p>The Elastic web crawler discovers and extracts web content, transforms it into structured documents, and indexes those documents into Elasticsearch.</p>
<p>Before you get started, decide which web content you&#8217;d like to crawl.
The iterative feedback cycle looks like this:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Defining the crawl
</li>
<li class="listitem">
Monitoring the results
</li>
<li class="listitem">
Troubleshooting
</li>
</ul>
</div>
<p>Repeat, as necessary, every time you make changes to the crawl configuration.
Get started by limiting your crawl to a single domain and a small number of documents.
When you are happy with the results at a small scale, you can gradually add more complexity.</p>
<p>Here you will learn how to:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Create an Elasticsearch index
</li>
<li class="listitem">
Add and validate domains
</li>
<li class="listitem">
Define entry points and sitemaps
</li>
<li class="listitem">
Define crawl rules
</li>
<li class="listitem">
Monitor crawls and perform basic troubleshooting
</li>
<li class="listitem">
Verify your documents
</li>
</ul>
</div>
<h4><a id="crawler-getting-started-prerequisites"></a>Prerequisites<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
An Elastic deployment running Elasticsearch, Kibana, and Enterprise Search (<span class="strong strong"><strong>Version 8.4</strong></span> or later).
</li>
<li class="listitem">
The Enterprise Search server must be enabled and running in your deployment.
</li>
<li class="listitem">
Ensure users have access to this feature.
</li>
</ul>
</div>
<h4><a id="crawler-getting-started-create-index"></a>Create an Elasticsearch index<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>The first step is to create an Elasticsearch index.
This is where the crawler will store indexed webpage content.</p>
<p>Follow these steps in Kibana:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span>.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Create new index</strong></span>.
</li>
<li class="listitem">
Use the <span class="strong strong"><strong>web crawler</strong></span> as ingestion method.
</li>
<li class="listitem">
Name your new index.
</li>
<li class="listitem">
Choose your document language.
</li>
</ul>
</div>
<p>You are now ready to add domains.</p>
<h4><a id="crawler-getting-started-domain"></a>Add domains<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>It&#8217;s time to specify the domains you want to crawl.
We recommend getting started with a single domain.</p>
<p>Follow these steps to validate and add domains:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Manage domains</strong></span> in <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span>.
</li>
<li class="listitem">
Enter a domain to be crawled.
The web crawler will not crawl any webpages outside of your defined domains.
</li>
<li class="listitem">
Review any warnings flagged by the crawler domain validation process.
</li>
<li class="listitem">
Inspect the domain&#8217;s <code class="literal">robots.txt</code> file, if it exists.
The instructions within this file, also called directives, communicate which paths within that domain are disallowed for crawling.
</li>
<li class="listitem">
Add the domain.
</li>
</ul>
</div>
<h4><a id="crawler-getting-started-entry-points-sitemaps"></a>Define entry points and sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<h5><a id="crawler-getting-started-entry-points"></a>Entry points<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<p>Each domain must have at least one entry point.
Entry points are the paths from which the crawler will start each crawl.
Ensure entry points for each domain are allowed by the domain&#8217;s crawl rules, and the directives within the domain&#8217;s <code class="literal">robots.txt</code> file.</p>
<p>Add multiple entries, if some pages are not discoverable from the first entry point.
For example, if your domain contains an “island” page that is not linked from other pages, simply add that full URL as an entry point.</p>
<h5><a id="crawler-getting-started-sitemaps"></a>Sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<p>If the website you are crawling uses sitemaps, you can specify the sitemap URLs.
Note that you can choose to submit URLs to the Enterprise Search web crawler using sitemaps, entry points, or a combination of both.</p>
<p>To add a sitemap to a domain you manage, you can specify it within a <code class="literal">robots.txt</code> file.
At the start of each crawl, the web crawler fetches and processes each domain&#8217;s <code class="literal">robots.txt</code> file and each sitemap specified within those files.</p>
<p>You may prefer to use sitemaps over entry points, because you have already published sitemaps for other web crawlers.</p>
<h4><a id="crawler-getting-started-crawl-rules"></a>Define crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>This is enough configuration to start crawling, but we recommend first defining a few <span class="strong strong"><strong>crawl rules</strong></span>.
A crawl rule is a crawler instruction to allow or disallow specific <span class="strong strong"><strong>paths</strong></span> within a domain.
It makes sense to start with test crawls across a limited subset of paths.</p>
<p>To define a crawl rule follow these steps:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Choose one of two <span class="strong strong"><strong>policies</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Allow
</li>
<li class="listitem">
Disallow
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Choose one of four <span class="strong strong"><strong>rules</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<em>Begins with</em>
</li>
<li class="listitem">
<em>Ends with</em>
</li>
<li class="listitem">
<em>Contains</em>
</li>
<li class="listitem">
<em>Regex</em>
</li>
</ul>
</div>
</li>
<li class="listitem">
Define the <span class="strong strong"><strong>path pattern</strong></span>, e.g. <code class="literal">/blog</code>, <code class="literal">/troubleshooting</code>, <code class="literal">.*</code>.
</li>
</ol>
</div>
<p>The Boolean logic of crawl rules can get complex fast.
Start simple, and when you are comfortable with the results, gradually add complexity as necessary.
For example, you might want to restrict your first crawls to a single path on the target domain.
Learn how to do this in the <a class="xref" href="crawler-getting-started.html#crawler-getting-started-crawl-rules-examples" title="Examples: Restricting paths using crawl rules">examples</a> below.</p>
<p>A few important facts about crawl rules:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
When the web crawler discovers a new page, it will evaluate the URL against the rules you defined — to decide if the URL should be visited.
</li>
<li class="listitem">
Rules are evaluated in sequential order.
The first crawl rule to match determines the policy for the URL.
Therefore, <span class="strong strong"><strong>order matters</strong></span>.
</li>
<li class="listitem">
Each URL is evaluated according to the first match.
</li>
<li class="listitem">
If the URL matches the crawl rule, the URL will be allowed or disallowed for crawling.
</li>
<li class="listitem">
If the URL doesn&#8217;t match the rule, it will be evaluated against the next rule.
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p><span class="strong strong"><strong>Default crawl rule</strong></span></p>
<p>The domain dashboard adds a default crawl rule to each domain, which cannot be deleted or re-ordered:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<h5><a id="crawler-getting-started-crawl-rules-examples"></a>Examples: Restricting paths using crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<p>Rule to disallow a specific path, e.g. <code class="literal">/blog</code>:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Rules to allow <em>only</em> the <code class="literal">/blog</code> path and disallow all others:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>When you restrict a crawl to specific paths, be sure to add entry points that allow the crawler to discover those paths.
The web crawler will crawl only those paths that are allowed by the crawl rules for the domain <span class="strong strong"><strong>and</strong></span> the directives within the <code class="literal">robots.txt</code> file for the domain.</p>
</div>
</div>
<h4><a id="crawler-getting-started-first-crawl"></a>Running your first crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>You have added and validated a domain, defined entry points, pointed to sitemaps, and set some basic crawl rules.
It&#8217;s time to run your first crawl.
It&#8217;s best to start with manual, one-off crawls before thinking about scheduling recurring crawls.</p>
<p><span class="strong strong"><strong>Crawl with custom settings</strong></span> is ideal for your first tests, because this allows you to further restrict the pages to be crawled.</p>
<p>Select the <span class="strong strong"><strong>Crawl with custom settings</strong></span> option and follow these steps:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>Set a maximum crawl depth, to specify how many pages deep the crawler traverses.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Set the value to <code class="literal">1</code>, for example, to limit the crawl to only entry points.
</li>
</ul>
</div>
</li>
<li class="listitem">
Crawl select domains.
</li>
</ul>
</div>
<p>This will help you define small, targeted test crawls.</p>
<p>It&#8217;s time to launch your crawl.
Next you&#8217;ll need to verify that the crawl is running, and that documents are being added to your index.</p>
<h4><a id="crawler-getting-started-monitor-crawl"></a>Monitor crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>Under <span class="strong strong"><strong>Recent crawl requests</strong></span>, you can quickly verify important information about your crawls.</p>
<p>Each crawl has an associated crawl request, identified by a unique ID in the following format: <code class="literal">60106315beae67d49a8e787d</code>.</p>
<p>Each crawl has a status, which quickly communicates its state.
If a crawl failed, you&#8217;ll need to <a class="xref" href="crawler-getting-started.html#crawler-getting-started-troubleshooting" title="Basic troubleshooting">troubleshoot</a>.
If a crawl is running, or has successfully completed, you can start <a class="xref" href="crawler-getting-started.html#crawler-getting-started-troubleshooting-verify-documents" title="Verify your documents">verifying your documents</a>.</p>
<h4><a id="crawler-getting-started-troubleshooting"></a>Basic troubleshooting<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>Here we outline some basic troubleshooting techniques you might encounter with your first crawls.
For advanced troubleshooting tips and tricks, including how to view web crawler logs in Kibana, see <a class="xref" href="crawler-troubleshooting.html" title="Troubleshooting crawls">Troubleshooting crawls</a>.</p>
<h5><a id="crawler-getting-started-troubleshooting-content-discovery"></a>Content discovery and extraction problems<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<p>You may encounter some of the following issues:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><span class="strong strong"><strong>A domain is not being crawled:</strong></span>  Make sure you&#8217;ve added it.
Each unique combination of protocol and hostname is a separate domain. Each of the following is its <span class="strong strong"><strong>own domain</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">http://example.com</code>
</li>
<li class="listitem">
<code class="literal">https://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">http://shop.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://shop.example.com</code>
</li>
</ul>
</div>
</li>
<li class="listitem">
<span class="strong strong"><strong>A path is not being crawled:</strong></span> Make sure it&#8217;s not disallowed by crawl rules or <code class="literal">robots.txt</code> file.
</li>
<li class="listitem">
<span class="strong strong"><strong>A page has no incoming links:</strong></span> Add the URL as an entry point.
</li>
<li class="listitem">
<span class="strong strong"><strong>A page is too large:</strong></span> The web crawler will not index its content.
</li>
<li class="listitem">
<span class="strong strong"><strong>A page is not indexed:</strong></span> The web crawler cannot parse extremely broken HTML pages.
</li>
<li class="listitem">
<span class="strong strong"><strong>Duplicate content:</strong></span> The web crawler only indexes unique pages.
</li>
</ul>
</div>
<h5><a id="crawler-getting-started-troubleshooting-verify-documents"></a>Verify your documents<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Examine the fields that make up an indexed document.
The web crawler transforms all HTML page content into documents with these same fields.</p>
</div>
</div>
<p>If a crawl is running successfully, you&#8217;ll start to see documents being added to your Elasticsearch index almost immediately.
You need to check that these documents are being indexed as expected.</p>
<p>Follow this checklist to verify your documents:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Browse the documents in your index, under <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span> &#8594; <span class="strong strong"><strong>Documents</strong></span>.
</li>
<li class="listitem">
Find a specific document, by wrapping the document’s URL in quotes.
For example: <code class="literal">"https://example.com/some/page.html"</code>.
</li>
<li class="listitem">
Confirm that documents contain the expected content.
Does each document have the content you were expecting?
</li>
<li class="listitem">
Check for missing documents.
</li>
<li class="listitem">
Check for the presence of documents which shouldn&#8217;t be indexed.
</li>
</ul>
</div>
<p>Your documents may contain too much, or too little, content.
If you&#8217;re crawling content you control, you may need to <a class="xref" href="crawler-content.html" title="Optimizing web content for the web crawler">optimize your content</a>.</p>
<p>If you get too many documents, in your next iteration either delete the documents you don&#8217;t want, or start over with a new index.</p>
<h5><a id="crawler-getting-started-troubleshooting-verify-documents-dev-tools"></a>Kibana Dev Tools<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h5>
<p>Your documents are stored in Elasticsearch indices, so you can also explore them using <a href="/guide/en/kibana/master/devtools-kibana.html" class="ulink" target="_blank" rel="noopener">Kibana Dev Tools</a>.</p>
<p>Use the Elasticsearch REST API in <span class="strong strong"><strong>Console</strong></span> to send requests and view their responses.</p>
<p>See the <a href="/guide/en/elasticsearch/reference/master/search-your-data.html" class="ulink" target="_top">Elasticsearch documentation</a> to learn all about how to search your data.</p>
<h4><a id="crawler-getting-started-next-steps"></a>Next steps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-getting-started.asciidoc">edit</a></h4>
<p>You&#8217;ve learned how to set up, configure, and troubleshoot your first crawls using the web crawler.
You&#8217;ve run a few targeted test crawls.
You know how to define basic crawl rules and how to verify your crawled documents.
You&#8217;ve followed the iterative feedback cycle of <span class="strong strong"><strong>Defining the crawl</strong></span> → <span class="strong strong"><strong>Monitoring the results</strong></span> → <span class="strong strong"><strong>Troubleshooting</strong></span>, to gradually add complexity to your settings.</p>
<p>Once your documents are being crawled and indexed as required,  you&#8217;re ready to dive into advanced crawler configuration topics.</p>
<p>Learn more:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-searching.html" title="Searching your crawled documents">Searching your crawled documents</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html" title="Managing crawls in Kibana">Managing crawls in Kibana</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html" title="Optimizing web content for the web crawler">Optimizing web content for the web crawler</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-troubleshooting.html" title="Troubleshooting crawls">Troubleshooting crawls</a>
</li>
</ul>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-searching.html">Searching your crawled documents »</a>
</span>
</div>
</div>
</body>
</html>
