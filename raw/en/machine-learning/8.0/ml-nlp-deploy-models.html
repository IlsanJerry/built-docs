<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="description" content="You can import trained models into your cluster and configure them for specific NLP tasks.">
<meta name="keywords" content="ML, Elastic Stack, natural language processing">
<title>Deploy trained models | Machine Learning in the Elastic Stack [8.0] | Elastic</title>
<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [8.0]"/>
<link rel="up" href="ml-nlp.html" title="Natural language processing"/>
<link rel="prev" href="ml-nlp-search-compare.html" title="Search and compare text"/>
<link rel="next" href="ml-nlp-inference.html" title="Add NLP inference to ingest pipelines"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/8.0"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="8.0"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [8.0]</a></span>
»
<span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
»
<span class="breadcrumb-node">Deploy trained models</span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-search-compare.html">« Search and compare text</a>
</span>
<span class="next">
<a href="ml-nlp-inference.html">Add NLP inference to ingest pipelines »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-nlp-deploy-models"></a>Deploy trained models<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/8.0/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h2>
</div></div></div>
<p>If you want to perform natural language processing tasks in your cluster, you must deploy an
appropriate trained model. There are Elasticsearch APIs and tooling support in
<a href="https://github.com/elastic/eland" class="ulink" target="_top">Eland</a> to prepare and manage models.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="ml-nlp-deploy-models.html#ml-nlp-select-model" title="Select a trained model">Select a trained model</a>.
</li>
<li class="listitem">
<a class="xref" href="ml-nlp-deploy-models.html#ml-nlp-import-model" title="Import the trained model and vocabulary">Import the trained model and vocabulary</a>.
</li>
<li class="listitem">
<a class="xref" href="ml-nlp-deploy-models.html#ml-nlp-deploy-model" title="Deploy the model in your cluster">Deploy the model in your cluster</a>.
</li>
<li class="listitem">
<a class="xref" href="ml-nlp-deploy-models.html#ml-nlp-test-inference" title="Test the deployment">Test the deployment</a>.
</li>
</ol>
</div>
<h3><a id="ml-nlp-select-model"></a>Select a trained model<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/8.0/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h3>
<p>Per the <a class="xref" href="ml-nlp-overview.html" title="Overview"><em>Overview</em></a>, there are multiple ways that you can use NLP
features within the Elastic Stack. After you determine which type of NLP task you want
to perform, you must choose an appropriate trained model. You can either create
the model yourself or find one that meets your needs.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>The Elastic Stack machine learning features support transformer models that conform to
the standard BERT model interface and use the WordPiece tokenization algorithm.</p>
</div>
</div>
<p>The simplest method is to use a model that has already been fine-tuned for the
type of analysis that you want to perform. For example, there are models and
data sets available for specific NLP tasks on
<a href="https://huggingface.co/models" class="ulink" target="_top">Hugging Face</a>.</p>
<p>If you choose to perform language identification by using
the <code class="literal">lang_ident_model_1</code> that is provided in the cluster, no further steps are
required to import or deploy the model.</p>
<h3><a id="ml-nlp-import-model"></a>Import the trained model and vocabulary<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/8.0/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h3>
<p>After you find or create a model, you must import it and its tokenizer
vocabulary to your cluster.</p>
<p>Trained models must be in a TorchScript representation for use with
Elastic Stack machine learning features. To use scripts that convert Hugging Face transformer models
to their TorchScript representations and import them with their tokenizer
vocabularies, refer to <a href="https://github.com/elastic/eland#nlp-with-pytorch" class="ulink" target="_top">https://github.com/elastic/eland#nlp-with-pytorch</a>.</p>
<p>If you used PyTorch to create your own trained model, you must create a
TorchScript representation of the model. Then you can use eland to import the
necessary artifacts into your cluster.</p>
<p>Alternatively, you can use the
<a href="/guide/en/elasticsearch/reference/8.0/put-trained-model-vocabulary.html" class="ulink" target="_top">create trained model vocabulary API</a> and
<a href="/guide/en/elasticsearch/reference/8.0/put-trained-models.html" class="ulink" target="_top">create trained models API</a> and
<a href="/guide/en/elasticsearch/reference/8.0/put-trained-model-definition-part.html" class="ulink" target="_top">create trained model definition part API</a>.
When you import the model, however, it must be chunked and imported one chunk at
a time for storage in parts due its size. Since eland encapsulates this process
in a single Python method, it is the recommended import method.</p>
<h3><a id="ml-nlp-deploy-model"></a>Deploy the model in your cluster<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/8.0/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h3>
<p>After you import the model and vocabulary, you can use Kibana to view and manage
their deployment across your cluster under <span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Model Management</strong></span>.
Alternatively, you can use the
<a href="/guide/en/elasticsearch/reference/8.0/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment API</a>.</p>
<p>When you deploy the model, it is allocated to all available machine learning nodes. The
model is loaded into memory in a native process that encapsulates <code class="literal">libtorch</code>,
which is the underlying machine learning library of PyTorch.</p>
<p>You can optionally specify the number of CPU cores it has access to on each node.
If you choose to optimize for latency (that is to say, inference should return
as fast as possible), you can increase <code class="literal">inference_threads</code> to lower latencies.
If you choose to optimize for throughput (that is, maximize the number of
parallel inference requests), you can increase <code class="literal">model_threads</code> to increase
throughput. In general, the total size of threading settings across all models
on a node should not exceed the number of physical CPU cores available on the
node, minus one (for non-inference operations). In Elastic Cloud environments, the
core count is virtualized CPUs (vCPUs) and this total size should typically be
no more than half the available vCPUs, minus one.</p>
<p>You can view the allocation status in Kibana or by using the
<a href="/guide/en/elasticsearch/reference/8.0/get-trained-models-stats.html" class="ulink" target="_top">get trained model stats API</a>.</p>
<h3><a id="ml-nlp-test-inference"></a>Test the deployment<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/8.0/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h3>
<p>When the model is deployed on at least one node in the cluster, you can begin to
perform inference. <em>Inference</em> is a machine learning feature that enables you to use your
trained models to perform NLP tasks (such as text extraction, classification, or
embeddings) on incoming data.</p>
<p>The simplest method to test your model against new data is to use the
<a href="/guide/en/elasticsearch/reference/8.0/infer-trained-model-deployment.html" class="ulink" target="_top">infer trained model deployment API</a>.</p>
<p>If you are satisfied with the results, you can add these NLP tasks into
<a class="xref" href="ml-nlp-inference.html" title="Add NLP inference to ingest pipelines">ingestion pipelines</a>.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="ml-nlp-search-compare.html">« Search and compare text</a>
</span>
<span class="next">
<a href="ml-nlp-inference.html">Add NLP inference to ingest pipelines »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
