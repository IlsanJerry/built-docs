<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title xmlns="">标准分词器
        | Elasticsearch: 权威指南
      | Elastic
    </title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch: 权威指南" /><link rel="up" href="identifying-words.html" title="词汇识别" /><link rel="prev" href="standard-analyzer.html" title="标准分析器" /><link rel="next" href="icu-plugin.html" title="安装 ICU 插件" /><meta xmlns="" name="description" content="有关如何使用 Elasticsearch、Kibana、Logstash、Beats、X-Pack、Elastic Cloud、Elasticsearch for Apache Hadoop 及我们各种语言的客户端的文档。" /><meta xmlns="" name="DC.type" content="Learn/Docs/Elasticsearch/Definitive Guide" /><meta xmlns="" name="DC.subject" content="Elasticsearch" /><meta xmlns="" name="DC.identifier" content="cn" /></head><body><div xmlns="" class="page_header"><b>请注意:</b><br/>本书基于 Elasticsearch 2.x 版本，有些内容可能已经过时。
</div><div xmlns="" class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch: 权威指南</a></span> » <span class="breadcrumb-link"><a href="languages.html">处理人类语言</a></span> » <span class="breadcrumb-link"><a href="identifying-words.html">词汇识别</a></span> » <span class="breadcrumb-node">标准分词器</span></div><div xmlns="" class="navheader"><span class="prev"><a href="standard-analyzer.html">
              « 
              标准分析器</a>
           
        </span><span class="next">
           
          <a href="icu-plugin.html">安装 ICU 插件
               »
            </a></span></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="standard-tokenizer"></a>标准分词器<a xmlns="" href="https://github.com/elasticsearch-cn/elasticsearch-definitive-guide/edit/cn/210_Identifying_words/20_Standard_tokenizer.asciidoc" class="edit_me" title="在 GitHub 上编辑本页" rel="nofollow">编辑</a></h2></div></div></div><p><span class="emphasis"><em>分词器</em></span> 接受一个字符串作为输入，将<a id="id-1.6.4.8.2.2" class="indexterm"></a>
<a id="id-1.6.4.8.2.3" class="indexterm"></a>
<a id="id-1.6.4.8.2.4" class="indexterm"></a><a id="id-1.6.4.8.2.5" class="indexterm"></a><a id="id-1.6.4.8.2.6" class="indexterm"></a>这个字符串拆分成独立的词或 <span class="emphasis"><em>语汇单元（token）</em></span>
（可能会丢弃一些标点符号等字符），然后输出一个 <span class="emphasis"><em>语汇单元流（token stream）</em></span> 。</p><p>有趣的是用于词汇 <span class="emphasis"><em>识别</em></span> 的算法。 <code class="literal">whitespace</code> （空白字符）分词器<a id="id-1.6.4.8.3.3" class="indexterm"></a>按空白字符 —— 空格、tabs、换行符等等进行简单拆分 —— 然后假定连续的非空格字符组成了一个语汇单元。例如：</p><div xmlns="" class="pre_wrapper lang-js"><pre xmlns="http://www.w3.org/1999/xhtml" class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=whitespace
You're the 1st runner home!</pre></div><p>这个请求会返回如下词项（terms）：
<code class="literal">You're</code> 、 <code class="literal">the</code> 、 <code class="literal">1st</code> 、 <code class="literal">runner</code> 、 <code class="literal">home!</code></p><p><code class="literal">letter</code> 分词器 ，采用另外一种策略，按照任何非字符进行拆分，
这样<a id="id-1.6.4.8.6.2" class="indexterm"></a>将会返回如下单词： <code class="literal">You</code> 、 <code class="literal">re</code> 、 <code class="literal">the</code> 、 <code class="literal">st</code> 、 <code class="literal">runner</code> 、 <code class="literal">home</code> 。</p><p><code class="literal">standard</code> 分词器<a id="id-1.6.4.8.7.2" class="indexterm"></a>使用 Unicode 文本分割算法
（定义来源于 <a class="ulink" href="http://unicode.org/reports/tr29/" target="_top">Unicode Standard Annex #29</a>）来寻找单词 <span class="emphasis"><em>之间</em></span> 的界限，并且输出所有界限之间的内容。
Unicode 内含的知识使其可以成功的对包含混合语言的文本进行分词。</p><p>标点符号<a id="id-1.6.4.8.8.1" class="indexterm"></a>
<a id="id-1.6.4.8.8.2" class="indexterm"></a>可能是单词的一部分，也可能不是，这取决于它出现的位置：</p><div xmlns="" class="pre_wrapper lang-js"><pre xmlns="http://www.w3.org/1999/xhtml" class="programlisting prettyprint lang-js">GET /_analyze?tokenizer=standard
You're my 'favorite'.</pre></div><p>在这个例子中，<code class="literal">You're</code> 中的撇号被视为单词的一部分，然而 <code class="literal">'favorite'</code> 中的单引号则不会被视为单词的一部分，
所以分词结果如下： <code class="literal">You're</code> 、 <code class="literal">my</code> 、 <code class="literal">favorite</code> 。</p><div xmlns="" class="tip admon"><div class="icon"><img alt="提示" src="images/icons/tip.png" /></div><div class="admon_content"><p xmlns="http://www.w3.org/1999/xhtml"><code class="literal">uax_url_email</code> 分词器<a id="id-1.6.4.8.11.1.2" class="indexterm"></a>和 <code class="literal">standard</code> 分词器工作方式极其相同。
区别只在于它能识别<a id="id-1.6.4.8.11.1.4" class="indexterm"></a> email 地址和 URLs 并输出为单个语汇单元。
<code class="literal">standard</code> 分词器则不一样，会将 email 地址和 URLs 拆分成独立的单词。
例如，email 地址 <code class="literal">joe-bloggs@foo-bar.com</code> 的分词结果为 <code class="literal">joe</code> 、 <code class="literal">bloggs</code> 、 <code class="literal">foo</code> 、 <code class="literal">bar.com</code> 。</p></div></div><p><code class="literal">standard</code> 分词器是大多数语言分词的一个合理的起点，特别是西方语言。
事实上，它构成了大多数特定语言分析器的基础，如 <code class="literal">english</code> 、<code class="literal">french</code> 和 <code class="literal">spanish</code> 分析器。
它也支持亚洲语言，只是有些缺陷，你可以考虑通过 ICU 插件的方式使用 <code class="literal">icu_tokenizer</code> <a id="id-1.6.4.8.12.6" class="indexterm"></a>进行替换。</p></div><div xmlns="" class="navfooter"><span class="prev"><a href="standard-analyzer.html">
              « 
              标准分析器</a>
           
        </span><span class="next">
           
          <a href="icu-plugin.html">安装 ICU 插件
               »
            </a></span></div></body></html>